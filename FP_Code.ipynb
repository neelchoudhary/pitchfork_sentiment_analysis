{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FP_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "author": [
      {
        "@type": "Person",
        "name": "Kevin Gold"
      }
    ]
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2itSY-L1zv6K"
      },
      "source": [
        "# Final Project: Pitchfork Album Score Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKb47fhOzzDM"
      },
      "source": [
        "We are attempting to classify pitchfork album reviews by using multiple machine learning methods for Sentiment Analysis. We started by using a Naive Bayes Model, similar to HW03, to classify album reviews as either neutral or positive. Unsurprisingly, it did not perform that well, mainly due to the fact that pitchfork reviews use a higher level of vocabulary and more complex sentence structure than an average yelp review (as seen in hw03). So we used the results from the Naive Bayes model as a baseline to compare against. We then attempted to train an SVM on our data for sentiment analysis. After tweaking the parameters, we were able to achieve a somewhat decent accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oKlSSwtwH2r"
      },
      "source": [
        "**To run this code, press Run all in Google Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlIxFbCze-WX"
      },
      "source": [
        "## The Domain\n",
        "Our domain was album reviews from the website [Pitchfork](https://pitchfork.com/) - the self acclaimed 'most trusted voice in music'. Pitchfork is known for their critical reviews that can make or break careers. Their reviews are known for their flowery, superfluous language which makes it more difficult to perform sentiment analysis on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1Nr9eCdzyf"
      },
      "source": [
        "## The Data\n",
        "\n",
        "We obtained our data from [Kaggle](https://www.kaggle.com/nolanbconaway/pitchfork-data). Kaggle user Nolan Conaway conveniently scraped and compiled this data from [Pitchfork reviews](https://pitchfork.com/reviews/albums/). He scraped data starting from Jan 5, 1999 to Jan 8, 2017. This gave us album review data spanning over 18 years, which yielded 18,401 rows of data - more than enough for most machine learning applications. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtCqfk-v0lQa"
      },
      "source": [
        "import pandas as pd\n",
        "raw_df = pd.read_csv('https://raw.githubusercontent.com/neelchoudhary/pitchfork_sentiment_analysis/master/data.csv', engine='python')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpduykvfStSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "b4ea676c-4f6c-4018-f11a-89c87f616051"
      },
      "source": [
        "raw_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>title</th>\n",
              "      <th>artist</th>\n",
              "      <th>url</th>\n",
              "      <th>score</th>\n",
              "      <th>best_new_music</th>\n",
              "      <th>author</th>\n",
              "      <th>author_type</th>\n",
              "      <th>pub_date</th>\n",
              "      <th>pub_weekday</th>\n",
              "      <th>pub_day</th>\n",
              "      <th>pub_month</th>\n",
              "      <th>pub_year</th>\n",
              "      <th>reviewid.1</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22703</td>\n",
              "      <td>mezzanine</td>\n",
              "      <td>massive attack</td>\n",
              "      <td>http://pitchfork.com/reviews/albums/22703-mezz...</td>\n",
              "      <td>9.3</td>\n",
              "      <td>0</td>\n",
              "      <td>nate patrin</td>\n",
              "      <td>contributor</td>\n",
              "      <td>2017-01-08</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>22703</td>\n",
              "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22721</td>\n",
              "      <td>prelapsarian</td>\n",
              "      <td>krallice</td>\n",
              "      <td>http://pitchfork.com/reviews/albums/22721-prel...</td>\n",
              "      <td>7.9</td>\n",
              "      <td>0</td>\n",
              "      <td>zoe camp</td>\n",
              "      <td>contributor</td>\n",
              "      <td>2017-01-07</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>22721</td>\n",
              "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22659</td>\n",
              "      <td>all of them naturals</td>\n",
              "      <td>uranium club</td>\n",
              "      <td>http://pitchfork.com/reviews/albums/22659-all-...</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0</td>\n",
              "      <td>david glickman</td>\n",
              "      <td>contributor</td>\n",
              "      <td>2017-01-07</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>22659</td>\n",
              "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22661</td>\n",
              "      <td>first songs</td>\n",
              "      <td>kleenex, liliput</td>\n",
              "      <td>http://pitchfork.com/reviews/albums/22661-firs...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1</td>\n",
              "      <td>jenn pelly</td>\n",
              "      <td>associate reviews editor</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>22661</td>\n",
              "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22725</td>\n",
              "      <td>new start</td>\n",
              "      <td>taso</td>\n",
              "      <td>http://pitchfork.com/reviews/albums/22725-new-...</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0</td>\n",
              "      <td>kevin lozano</td>\n",
              "      <td>tracks coordinator</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>22725</td>\n",
              "      <td>It is impossible to consider a given release b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   reviewid  ...                                            content\n",
              "0     22703  ...  “Trip-hop” eventually became a ’90s punchline,...\n",
              "1     22721  ...  Eight years, five albums, and two EPs in, the ...\n",
              "2     22659  ...  Minneapolis’ Uranium Club seem to revel in bei...\n",
              "3     22661  ...  Kleenex began with a crash. It transpired one ...\n",
              "4     22725  ...  It is impossible to consider a given release b...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNCkfCt_hmpp"
      },
      "source": [
        "## Data Cleaning\n",
        "As seen in the raw data, there are many columns that are not needed for sentiment analysis. There are also too many possibilities for the score, so we rounded each score to the nearest integer. The following code block rounds the score and drops unnecessary columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "re4e4HOg_eAH",
        "outputId": "9a21579f-2928-4bef-e7ca-769acfc08581"
      },
      "source": [
        "rounded_df = raw_df.round({'score': 0})\n",
        "df = rounded_df.drop(columns=['best_new_music', 'pub_weekday', 'pub_day', 'pub_month', 'pub_year', 'author_type', 'reviewid.1', 'author', 'pub_date', 'url', 'artist', 'title']).dropna()\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>score</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22703</td>\n",
              "      <td>9.0</td>\n",
              "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22721</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22659</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22661</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22725</td>\n",
              "      <td>8.0</td>\n",
              "      <td>It is impossible to consider a given release b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   reviewid  score                                            content\n",
              "0     22703    9.0  “Trip-hop” eventually became a ’90s punchline,...\n",
              "1     22721    8.0  Eight years, five albums, and two EPs in, the ...\n",
              "2     22659    7.0  Minneapolis’ Uranium Club seem to revel in bei...\n",
              "3     22661    9.0  Kleenex began with a crash. It transpired one ...\n",
              "4     22725    8.0  It is impossible to consider a given release b..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Xqs1bHjtx6"
      },
      "source": [
        "## Data Visualization\n",
        "We analyzed the score distribution of our data in order to figure out the best way to cull / transform our data in order to make it more balanced. This is to avoid issues with overfitting to a certain score that is more prevelant than the other scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIuEjwcPkQS9"
      },
      "source": [
        "###Score distribution graph using Seaborn\n",
        "As seen below, the score is heavily skewed towards the 7-8 range. This is problematic as there is very little data at the tails, especially the left tail of this distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "JYuFk1j5j7di",
        "outputId": "824d17fb-ccd2-4726-cdfc-c08c38b1fbcb"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x=df.score)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f80193d8b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rawrurb9lf16"
      },
      "source": [
        "Given these results, we decided to cull data with a rounded score less than 5. We then decided to consolidate the scores 5-7 into the neutral category (0), and the scores 8-10 in the positive category (1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxge-9OYliB_"
      },
      "source": [
        "def score_transform(rating):\n",
        "  rating = float(rating)\n",
        "  if rating < 5:\n",
        "    return -1 # negative\n",
        "  if rating >= 5 and rating <= 7:\n",
        "    return 0 # neutral\n",
        "  else:\n",
        "    return 1 # positive\n",
        "\n",
        "df_final = df.copy(deep=True)\n",
        "df_final['score'] = df['score'].apply(score_transform)\n",
        "df_final = df_final[df_final['score'] != -1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0IBr1DU-l6uy",
        "outputId": "c72fd32b-7fe5-43d6-e36f-a5f45e245fe7"
      },
      "source": [
        "df_final.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>score</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22703</td>\n",
              "      <td>1</td>\n",
              "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22721</td>\n",
              "      <td>1</td>\n",
              "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22659</td>\n",
              "      <td>0</td>\n",
              "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22661</td>\n",
              "      <td>1</td>\n",
              "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22725</td>\n",
              "      <td>1</td>\n",
              "      <td>It is impossible to consider a given release b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   reviewid  score                                            content\n",
              "0     22703      1  “Trip-hop” eventually became a ’90s punchline,...\n",
              "1     22721      1  Eight years, five albums, and two EPs in, the ...\n",
              "2     22659      0  Minneapolis’ Uranium Club seem to revel in bei...\n",
              "3     22661      1  Kleenex began with a crash. It transpired one ...\n",
              "4     22725      1  It is impossible to consider a given release b..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybUdDHNTmqGZ"
      },
      "source": [
        "The tranformed data is more balanced, as seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "3sD9L6gcmqjY",
        "outputId": "4896fff5-e0bd-408e-95c7-9f5e78df54d3"
      },
      "source": [
        "sns.countplot(x=df_final.score)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8019356c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNQHSUUKnkrP"
      },
      "source": [
        "## Features, Targets, and Data Splitting\n",
        "We split our data into three datasets for training, testing, and validation. We used a 60/20/20 split respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5MQBgrVNxcl"
      },
      "source": [
        "features = df_final.drop(columns='score')\n",
        "targets = df_final['score']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BSpWJADUoHzP",
        "outputId": "6e9422e1-9588-42c1-a5f6-38e3b53df90d"
      },
      "source": [
        "features.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22703</td>\n",
              "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22721</td>\n",
              "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22659</td>\n",
              "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22661</td>\n",
              "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22725</td>\n",
              "      <td>It is impossible to consider a given release b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   reviewid                                            content\n",
              "0     22703  “Trip-hop” eventually became a ’90s punchline,...\n",
              "1     22721  Eight years, five albums, and two EPs in, the ...\n",
              "2     22659  Minneapolis’ Uranium Club seem to revel in bei...\n",
              "3     22661  Kleenex began with a crash. It transpired one ...\n",
              "4     22725  It is impossible to consider a given release b..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx0fEUGyuUG9",
        "outputId": "3cdfba78-6ecc-46c7-9c6d-cb870e6b3224"
      },
      "source": [
        "targets.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    1\n",
              "2    0\n",
              "3    1\n",
              "4    1\n",
              "Name: score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t0Tgaa9wZXm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "features_trainvalidation, features_test, targets_trainvalidation, targets_test = train_test_split(features, targets, train_size=0.8, random_state=4100)\n",
        "features_train, features_validation, targets_train, targets_validation = train_test_split(features_trainvalidation, targets_trainvalidation, train_size=0.75, random_state=4100)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EDvsEzKjweHG",
        "outputId": "362b1d5d-954c-4dad-b5df-f0f3c02d9954"
      },
      "source": [
        "training_df = features_train.copy(deep=True)\n",
        "training_df['score'] = targets_train\n",
        "training_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9772</th>\n",
              "      <td>12187</td>\n",
              "      <td>So an MC who's been recording for over a decad...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6120</th>\n",
              "      <td>15928</td>\n",
              "      <td>There's been no shortage of dialogue around ho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5105</th>\n",
              "      <td>17154</td>\n",
              "      <td>On the last day of November 2010, Bonnie \"Prin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16716</th>\n",
              "      <td>6226</td>\n",
              "      <td>In the liner notes for O.S.T., People Under th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9035</th>\n",
              "      <td>12949</td>\n",
              "      <td>Unlike his predecessors in the fraternal  orde...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       reviewid                                            content  score\n",
              "9772      12187  So an MC who's been recording for over a decad...      1\n",
              "6120      15928  There's been no shortage of dialogue around ho...      1\n",
              "5105      17154  On the last day of November 2010, Bonnie \"Prin...      1\n",
              "16716      6226  In the liner notes for O.S.T., People Under th...      1\n",
              "9035      12949  Unlike his predecessors in the fraternal  orde...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qAvWQLun0dlf",
        "outputId": "f39fbcbe-c691-432c-f1e9-8c531324b0e3"
      },
      "source": [
        "validation_df = features_validation.copy(deep=True)\n",
        "validation_df['score'] = targets_validation\n",
        "validation_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3581</th>\n",
              "      <td>18705</td>\n",
              "      <td>The worst thing about the remarkable The First...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>721</th>\n",
              "      <td>21872</td>\n",
              "      <td>Try as they might, Mourn couldn’t help but sho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12685</th>\n",
              "      <td>7729</td>\n",
              "      <td>On paper, The Seeger Sessions seems like a ter...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10816</th>\n",
              "      <td>10754</td>\n",
              "      <td>\\r\\n    Getting the full picture of Joe Willia...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16114</th>\n",
              "      <td>5031</td>\n",
              "      <td>Friends Forever isn't a metal record, but let'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       reviewid                                            content  score\n",
              "3581      18705  The worst thing about the remarkable The First...      1\n",
              "721       21872  Try as they might, Mourn couldn’t help but sho...      0\n",
              "12685      7729  On paper, The Seeger Sessions seems like a ter...      1\n",
              "10816     10754  \\r\\n    Getting the full picture of Joe Willia...      1\n",
              "16114      5031  Friends Forever isn't a metal record, but let'...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zr_CZMoSYU76",
        "outputId": "2b759912-8fd2-4fdb-cad0-7b200a682efa"
      },
      "source": [
        "test_df = features_test.copy(deep=True)\n",
        "test_df['score'] = targets_test\n",
        "test_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewid</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>22212</td>\n",
              "      <td>The hippies inspired a wholesale revision of m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>21143</td>\n",
              "      <td>VHÖL are the ideal modern metal band—they fuse...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7035</th>\n",
              "      <td>14995</td>\n",
              "      <td>Sic Alps' simple garage-rock is both enticing ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12078</th>\n",
              "      <td>9538</td>\n",
              "      <td>While it's nice to hear Mac McCaughan hearken ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17236</th>\n",
              "      <td>223</td>\n",
              "      <td>Stop me if this gets sappy. And it might. Beca...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       reviewid                                            content  score\n",
              "402       22212  The hippies inspired a wholesale revision of m...      1\n",
              "1439      21143  VHÖL are the ideal modern metal band—they fuse...      1\n",
              "7035      14995  Sic Alps' simple garage-rock is both enticing ...      1\n",
              "12078      9538  While it's nice to hear Mac McCaughan hearken ...      1\n",
              "17236       223  Stop me if this gets sappy. And it might. Beca...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v31kQ88XrRdS"
      },
      "source": [
        "## Naive Bayes & Markov Model\n",
        "As a baseline, we used the naive bayes & markov model from HW03 to classify sentiments in our dataset. We modified to code to classify between a neutral and positive sentiment. We used bigrams for the Markov Model. We hypothesized that while both models would perform poorly on this dataset, the markov model would perform slightly better. This is because these album reviews are highly dependent on context, therefore, using bigrams would result in slightly higher performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aQBHsQfTiUp"
      },
      "source": [
        "Code attribution from HW03. Modified to fit our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaeTUf9_UFj8"
      },
      "source": [
        "from nltk.util import bigrams\n",
        "\n",
        "def tokenize(sentence):\n",
        "    \"\"\" Returns list of tokens (strings) from the sentence.\n",
        "\n",
        "    Sets to lowercase and separates tokens by whitespace.\n",
        "\n",
        "    Args:\n",
        "        sentence (string):  the string to tokenize\n",
        "    \"\"\"\n",
        "    return [t.lower() for t in sentence.split()]\n",
        "\n",
        "class ModelInfo:\n",
        "    \"\"\" Contains all counts from the data necessary to do Naive Bayes.\n",
        "\n",
        "    Attributes:\n",
        "        word_counts (List[Dict[string,int]]):  counts of tokens, indexed by class\n",
        "        bigram_counts (List[Dict[string,int]]): as word_counts, but for bigrams\n",
        "        sentiment_counts (List[int]):  counts of sentences with each sentiment\n",
        "        total_words (List[int]):  counts of words in each sentiment\n",
        "        bigram_denoms (List[Dict[string,int]]):  counts of how often a token starts a bigram,\n",
        "                                                 again one per sentiment.\n",
        "        total_bigrams (List[int]): counts of total bigrams for each sentiment\n",
        "        total_examples (int):  total sentence count\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_counts = [{}, {}]\n",
        "        self.bigram_counts = [{}, {}]\n",
        "        self.sentiment_counts = [0, 0]\n",
        "        self.total_words = [0, 0]\n",
        "        self.bigram_denoms = [{}, {}]\n",
        "        self.total_bigrams = [0, 0]\n",
        "        self.total_examples = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        words = self.total_words\n",
        "        sentiments = self.sentiment_counts\n",
        "        return \"words by sentiment: {} sentences: {}\".format(words, sentiments)\n",
        "\n",
        "    def update_word_counts(self, sentence, sentiment):\n",
        "        \"\"\" Consume a sentence and update all counts.\n",
        "\n",
        "        To \"tokenize\" the sentence we'll make use of NLTK, a widely-used Python natural language\n",
        "        processing (NLP) library.  This will handle otherwise onerous tasks like separating periods\n",
        "        from their attached words.  (Unless the periods are decimal points ... it's more complex\n",
        "        than you might think.)  The result of tokenization is a list of individual strings that are\n",
        "        words or their equivalent.\n",
        "\n",
        "        Args:\n",
        "            sentence (string):  The example sentence.\n",
        "            sentiment (int):  The sentiment label.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the relevant dicts for the sentiment\n",
        "        s_word_counts = self.word_counts[sentiment]\n",
        "        s_bigram_counts = self.bigram_counts[sentiment]\n",
        "        s_bigram_denoms = self.bigram_denoms[sentiment]\n",
        "        tokens = tokenize(sentence)\n",
        "        for token in tokens:\n",
        "            self.total_words[sentiment] += 1\n",
        "            s_word_counts[token] = s_word_counts.get(token, 0) + 1\n",
        "        my_bigrams = bigrams(tokens)\n",
        "        for bigram in my_bigrams:\n",
        "            s_bigram_counts[bigram] = s_bigram_counts.get(bigram, 0) + 1\n",
        "            s_bigram_denoms[bigram[0]] = s_bigram_denoms.get(bigram[0], 0) + 1\n",
        "            self.total_bigrams[sentiment] += 1\n",
        "       \n",
        "def get_models(df):\n",
        "    \"\"\"Returns a model_info object, consuming dataframe for examples.\"\"\"\n",
        "    last_fresh = 0\n",
        "    info = ModelInfo()\n",
        "    for idx, line in df.iterrows():\n",
        "        try:\n",
        "            sentence_num = int(idx)\n",
        "            # if sentence_num <= last_fresh:\n",
        "                # continue\n",
        "            last_fresh = sentence_num\n",
        "            sentiment = int(line['score'])\n",
        "            info.sentiment_counts[sentiment] += 1\n",
        "            info.total_examples += 1\n",
        "            info.update_word_counts(line['content'], sentiment)\n",
        "        except ValueError:\n",
        "            # Some kind of bad input?  Unlikely with our provided data\n",
        "            continue\n",
        "    return info"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbDFkUPuVVGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ff86b9-54fa-491e-f142-77b5c56428e6"
      },
      "source": [
        "model = get_models(training_df)\n",
        "print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words by sentiment: [3550755, 3282454] sentences: [5878, 4591]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izfKDOyI1G45"
      },
      "source": [
        "Code attribution from HW03. Modified to fit our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVb-RjDmtU7I"
      },
      "source": [
        "\"\"\" Doing some Naive Bayes and Markov Models to do basic sentiment analysis.\n",
        "\n",
        "Format is PhraseID[unused]   SentenceID  Sentence[tokenized] Sentiment\n",
        "\n",
        "We'll only use the first line for each SentenceID, since the others are\n",
        "micro-analyzed phrases that would just mess up our counts.\n",
        "\n",
        "Sentiment is on a 2-point scale:\n",
        "0 - neutral\n",
        "1 - positive\n",
        "\n",
        "For each kind of model, we'll build one model per sentiment category.\n",
        "Following Bayesian logic, base rates matter for each category; if critics\n",
        "are often negative, that should be a good guess in the absence of other\n",
        "information.\n",
        "\n",
        "Training input is assumed to be in a file called \"train.tsv\"\n",
        "\n",
        "Test sentences are received via stdin (thus either interactively or with input redirection).\n",
        "Output for each line of input is the following:\n",
        "\n",
        "Naive Bayes classification (0-4)\n",
        "Naive Bayes most likely class's log probability (with default double digits/precision)\n",
        "Markov Model classification (0-4)\n",
        "Markov Model most likely class's log probability\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import math\n",
        "\n",
        "CLASSES = 2\n",
        "# Assume sentence numbering starts with this number in the file\n",
        "\n",
        "# Probability of either a unigram or bigram that hasn't been seen -\n",
        "# needs to be small enough that it's \"practically a rounding error\"\n",
        "OUT_OF_VOCAB_PROB = 0.0000000001\n",
        "\n",
        "\n",
        "def naive_bayes_classify(info, sentence):\n",
        "    \"\"\" Use a Naive Bayes model to return sentence's most likely classification and the log prob.\n",
        "\n",
        "    The log probability should be base e (natural log).\n",
        "\n",
        "    Args:\n",
        "        info (ModelInfo):  a ModelInfo containing the counts from the training data\n",
        "        sentence (string):  the test sentence to classify\n",
        "    \n",
        "    Returns:\n",
        "        int for the best sentiment\n",
        "        float for the best log probability (unscaled, just ln(prior * product of cond. probs))\n",
        "    \"\"\"\n",
        "\n",
        "    logSentimentProb = []\n",
        "    totalWords = sum(info.sentiment_counts)\n",
        "    for tokenizedTotalWords in info.sentiment_counts:\n",
        "      logSentimentProb.append(math.log(tokenizedTotalWords / totalWords))\n",
        "\n",
        "    for word in tokenize(sentence):\n",
        "      for index, tokenizedWordCount in enumerate(info.word_counts):\n",
        "        if word in tokenizedWordCount:\n",
        "          logSentimentProb[index] += math.log(tokenizedWordCount.get(word) / info.sentiment_counts[index])\n",
        "        else:\n",
        "          logSentimentProb[index] += math.log(OUT_OF_VOCAB_PROB)\n",
        "\n",
        "    maxLogVal = logSentimentProb[0]\n",
        "    maxLogIndex = 0;\n",
        "    for index, logSentiment in enumerate(logSentimentProb):\n",
        "      if logSentiment > maxLogVal:\n",
        "        maxLogVal = logSentiment\n",
        "        maxLogIndex = index\n",
        " \n",
        "    best_class = maxLogIndex\n",
        "    best_log_prob = max(logSentimentProb)\n",
        "    return best_class, best_log_prob\n",
        "\n",
        "\n",
        "def markov_model_classify(info, sentence):\n",
        "    \"\"\" Like naive_bayes_classify, but use a bigram model for the evidence.\n",
        "\n",
        "    The first word should still use a unigram probability to get started.\n",
        "    Notice the existence of bigram_denoms, which has very slight differences from word_counts.\n",
        "    Log probability is again base e.\n",
        "\n",
        "    Args:\n",
        "        info (ModelInfo):  a ModelInfo containing the counts from the training data\n",
        "        sentence (string):  the test sentence to classify\n",
        "\n",
        "    Returns:\n",
        "        int for the best sentiment\n",
        "        float for the best log probability (unscaled, just ln(prior * product of cond. probs))\n",
        "    \"\"\"\n",
        "\n",
        "    logSentimentProb = []\n",
        "    totalWords = sum(info.sentiment_counts)\n",
        "    for tokenizedTotalWords in info.sentiment_counts:\n",
        "      logSentimentProb.append(math.log(tokenizedTotalWords / totalWords))\n",
        "\n",
        "    firstWord = tokenize(sentence)[0]\n",
        "    for index, tokenizedWordCount in enumerate(info.word_counts):\n",
        "      if firstWord in tokenizedWordCount:\n",
        "        logSentimentProb[index] += math.log(tokenizedWordCount.get(firstWord) / info.sentiment_counts[index])\n",
        "      else:\n",
        "        logSentimentProb[index] += math.log(OUT_OF_VOCAB_PROB)\n",
        "\n",
        "    prevWord = firstWord\n",
        "    for idx, word in enumerate(tokenize(sentence)):\n",
        "      if (idx != 0):\n",
        "        for index, tokenizedBigramCount in enumerate(info.bigram_counts):\n",
        "          bigramTuple = (prevWord, word)\n",
        "          if bigramTuple in tokenizedBigramCount:\n",
        "            logSentimentProb[index] += math.log(tokenizedBigramCount.get(bigramTuple) / info.bigram_denoms[index].get(prevWord))\n",
        "          else:\n",
        "            logSentimentProb[index] += math.log(OUT_OF_VOCAB_PROB)\n",
        "        prevWord = word\n",
        "\n",
        "    maxLogVal = logSentimentProb[0]\n",
        "    maxLogIndex = 0;\n",
        "    for index, logSentiment in enumerate(logSentimentProb):\n",
        "      if logSentiment > maxLogVal:\n",
        "        maxLogVal = logSentiment\n",
        "        maxLogIndex = index\n",
        "\n",
        "    best_class = maxLogIndex\n",
        "    best_log_prob = maxLogVal\n",
        "    return best_class, best_log_prob"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd4UMA90yQh9"
      },
      "source": [
        "### Error functions for accuracy prediction\n",
        "We created error functions for the bayes and markov model to return the error between how the classifier classified the review, and the actual score given to the review. We then iterated through the validation data and summed up the error to find the overall accuracy. We used this for fine-tuning our hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4UvTi-tz4F6"
      },
      "source": [
        "def error_function_bayes(model, review_content, real_score):\n",
        "  predicted_score = naive_bayes_classify(model, review_content)[0]\n",
        "  diff = abs(predicted_score - real_score)\n",
        "  return diff"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF4MQhnT13vR"
      },
      "source": [
        "def error_function_markov(model, review_content, real_score):\n",
        "  predicted_score = markov_model_classify(model, review_content)[0]\n",
        "  diff = abs(predicted_score - real_score)\n",
        "  return diff"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1upFppGI1PPm",
        "outputId": "6b3295de-f42a-4253-871e-89532e3f2d15"
      },
      "source": [
        "total_err = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "  count += 1\n",
        "  total_err += error_function_bayes(model, row['content'], row['score'])\n",
        "\n",
        "print(\"Accuracy: \" + str(1 - (total_err / count)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5117478510028654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaxTJDH42oPe",
        "outputId": "a310f120-c1f5-4c00-acac-e6368387db60"
      },
      "source": [
        "total_err = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "  count += 1\n",
        "  total_err += error_function_markov(model, row['content'], row['score'])\n",
        "\n",
        "print(\"Accuracy: \" + str(1 - (total_err / count)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6246418338108882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHD_5UkZwlpd"
      },
      "source": [
        "### Results: \n",
        "The accuracy for Naive Bayes was 0.512.\n",
        "\n",
        "The accuracy for the Markov Model was 0.625\n",
        "\n",
        "Unsurprisingly, both models performed poorly on our dataset. This was just to get a baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE6qRiBIt0Pd"
      },
      "source": [
        "## SVM Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzyZ_9ETEvM5"
      },
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = float(rating)\n",
        "  if rating < 5:\n",
        "    return \"neg\"\n",
        "  if rating >= 5 and rating <= 7:\n",
        "    return \"neu\"\n",
        "  else:\n",
        "    return \"pos\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CML2CMc_b83P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bf86ec57-8006-4009-bcb6-8530c6373752"
      },
      "source": [
        "svmData_raw = training_df.copy(deep=True)\n",
        "svmData_indexed = svmData_raw.reset_index()\n",
        "svmData_train = svmData_indexed.drop(columns=['reviewid', 'index'])\n",
        "svmData_train.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So an MC who's been recording for over a decad...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>There's been no shortage of dialogue around ho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>On the last day of November 2010, Bonnie \"Prin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In the liner notes for O.S.T., People Under th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Unlike his predecessors in the fraternal  orde...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  score\n",
              "0  So an MC who's been recording for over a decad...      1\n",
              "1  There's been no shortage of dialogue around ho...      1\n",
              "2  On the last day of November 2010, Bonnie \"Prin...      1\n",
              "3  In the liner notes for O.S.T., People Under th...      1\n",
              "4  Unlike his predecessors in the fraternal  orde...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo3Q0leedJnI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "13ae4c4e-4ce4-4c04-d722-8f8bfd1dad05"
      },
      "source": [
        "svmData_raw_validation = validation_df.copy(deep=True)\n",
        "svmData_indexed_validation = svmData_raw_validation.reset_index()\n",
        "svmData_validation = svmData_indexed_validation.drop(columns=['reviewid', 'index'])\n",
        "svmData_validation.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The worst thing about the remarkable The First...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Try as they might, Mourn couldn’t help but sho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>On paper, The Seeger Sessions seems like a ter...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\r\\n    Getting the full picture of Joe Willia...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Friends Forever isn't a metal record, but let'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  score\n",
              "0  The worst thing about the remarkable The First...      1\n",
              "1  Try as they might, Mourn couldn’t help but sho...      0\n",
              "2  On paper, The Seeger Sessions seems like a ter...      1\n",
              "3  \\r\\n    Getting the full picture of Joe Willia...      1\n",
              "4  Friends Forever isn't a metal record, but let'...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "osSDse5FAA1F",
        "outputId": "1d4c2aa8-0e07-44e2-b3d1-6e3d3561d9aa"
      },
      "source": [
        "svmData_raw_test = test_df.copy(deep=True)\n",
        "svmData_indexed_test = svmData_raw_test.reset_index()\n",
        "svmData_test = svmData_indexed_test.drop(columns=['reviewid', 'index'])\n",
        "svmData_test.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The hippies inspired a wholesale revision of m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VHÖL are the ideal modern metal band—they fuse...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sic Alps' simple garage-rock is both enticing ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>While it's nice to hear Mac McCaughan hearken ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Stop me if this gets sappy. And it might. Beca...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  score\n",
              "0  The hippies inspired a wholesale revision of m...      1\n",
              "1  VHÖL are the ideal modern metal band—they fuse...      1\n",
              "2  Sic Alps' simple garage-rock is both enticing ...      1\n",
              "3  While it's nice to hear Mac McCaughan hearken ...      1\n",
              "4  Stop me if this gets sappy. And it might. Beca...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frK4Fyct0k20"
      },
      "source": [
        "# Inspired from https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def svm_model(vectorizer, data):\n",
        "  train_matrix = vectorizer.fit_transform(svmData_train['content'])\n",
        "  test_matrix = vectorizer.transform(svmData_validation['content'])\n",
        "\n",
        "  classifier_rbf = svm.SVC(kernel='rbf')\n",
        "  classifier_rbf.fit(train_matrix, svmData_train['score'])\n",
        "  prediction_rbf = classifier_rbf.predict(test_matrix)\n",
        "\n",
        "  report = classification_report(svmData_validation['score'], prediction_rbf, output_dict=True)\n",
        "  print('Positive Accuracy: ', report['1']['precision'])\n",
        "  print('Neutral Accuracy: ', report['0']['precision'])\n",
        "  return report"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrP5Qe1i9-NF"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgSWr0Wd_EOD",
        "outputId": "b4814a09-a445-4419-b523-c9e6a43bcac0"
      },
      "source": [
        "v1 = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
        "v2 = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = False, use_idf = True)\n",
        "v3 = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = False)\n",
        "v4 = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = False, use_idf = False)\n",
        "r1 = svm_model(v1, svmData_validation['content'])\n",
        "r2 = svm_model(v2, svmData_validation['content'])\n",
        "r3 = svm_model(v3, svmData_validation['content'])\n",
        "r4 = svm_model(v4, svmData_validation['content'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Accuracy:  0.7201336675020885\n",
            "Neutral Accuracy:  0.7112952464020933\n",
            "Positive Accuracy:  0.7077189939288812\n",
            "Neutral Accuracy:  0.6970474967907574\n",
            "Positive Accuracy:  0.6979969183359014\n",
            "Neutral Accuracy:  0.718065693430657\n",
            "Positive Accuracy:  0.6866614048934491\n",
            "Neutral Accuracy:  0.7058029689608637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7qlGPh963T"
      },
      "source": [
        "From these results, it is evident that sublinear=True and use_idf=True are the ideal settings for these two parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONaCTc7Y-UX_",
        "outputId": "c111687f-7587-459b-af3d-5de41b854d65"
      },
      "source": [
        "v5 = TfidfVectorizer(min_df = 5, max_df = 0.9, sublinear_tf = True, use_idf = True)\n",
        "v6 = TfidfVectorizer(min_df = 5, max_df = 0.7, sublinear_tf = True, use_idf = True)\n",
        "r5 = svm_model(v5, svmData_validation['content'])\n",
        "r6 = svm_model(v6, svmData_validation['content'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Accuracy:  0.7226890756302521\n",
            "Neutral Accuracy:  0.711304347826087\n",
            "Positive Accuracy:  0.7265100671140939\n",
            "Neutral Accuracy:  0.7136640557006092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf6DmLBY-m37"
      },
      "source": [
        "From these results, it is evident that 0.7 is the ideal setting for the max_df parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94H-odUB-vKB",
        "outputId": "3914f517-169f-4dfd-f9ee-a63419799bd7"
      },
      "source": [
        "v7 = TfidfVectorizer(min_df = 4, max_df = 0.7, sublinear_tf = True, use_idf = True)\n",
        "v8 = TfidfVectorizer(min_df = 6, max_df = 0.7, sublinear_tf = True, use_idf = True)\n",
        "r7 = svm_model(v7, svmData_validation['content'])\n",
        "r8 = svm_model(v8, svmData_validation['content'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Accuracy:  0.7264309764309764\n",
            "Neutral Accuracy:  0.712858384013901\n",
            "Positive Accuracy:  0.7272727272727273\n",
            "Neutral Accuracy:  0.7154081187254474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuI5CmhG_Dr6"
      },
      "source": [
        "From these results, its is evident that 6 is the ideal setting for the min_df parameter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKbYtjI__NvB"
      },
      "source": [
        "After hyperparameter tuning, the most optimal parameters are the following: \n",
        "\n",
        "  TfidfVectorizer(min_df = 6,\n",
        "                               max_df = 0.7,\n",
        "                               sublinear_tf = True,\n",
        "                               use_idf = True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7EXet5vA6WY",
        "outputId": "7cc18e6f-e993-49e0-af54-e78875734ae9"
      },
      "source": [
        "optimal_vectorizer = TfidfVectorizer(min_df = 6, max_df = 0.7, sublinear_tf = True, use_idf = True)\n",
        "r_optimal = svm_model(optimal_vectorizer, svmData_validation['content'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Accuracy:  0.7272727272727273\n",
            "Neutral Accuracy:  0.7154081187254474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVGItec8K4b"
      },
      "source": [
        "### Test Results\n",
        "After tuning the hyperparameters, we concluded the most optimal parameters are min_df = 5, max_df = 0.7, sublinear_tf = True, use_idf = True, and lowercase=True. \n",
        "\n",
        "We achieved the following accuracies: \n",
        "\n",
        "\n",
        "Positive Accuracy:  0.727\n",
        "\n",
        "Neutral Accuracy:  0.715"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgqGR52EI1Zc",
        "outputId": "3d7a1529-974e-4c2b-f049-c84c33baf035"
      },
      "source": [
        "r_optimal = svm_model(optimal_vectorizer, svmData_test['content'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Accuracy:  0.7272727272727273\n",
            "Neutral Accuracy:  0.7154081187254474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck3DTqnFhPAR"
      },
      "source": [
        "The SVM performed significantly better than the Naive Bayes & Markov Model. With an average accuracy of 0.721 vs 0.512 and 0.625 for Naive Bayes & Markov Model, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaeRiwYpDWpQ"
      },
      "source": [
        "## LSTM\n",
        "We tried to implement an LSTM based of this article: \n",
        "https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948\n",
        "\n",
        "However, it was too slow to run through our data set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b83k0XyiDZFI"
      },
      "source": [
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "raw_df = pd.read_csv('data.csv')\n",
        "\n",
        "raw_df = raw_df.dropna()\n",
        "raw_df['content'] = raw_df['content'].str.lower()\n",
        "from string import punctuation\n",
        "\n",
        "raw_df['content'] = raw_df['content'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "\n",
        "review_list = raw_df['content'].tolist()\n",
        "\n",
        "train_on_gpu = False\n",
        "\n",
        "\n",
        "# tokenize labels\n",
        "labels = raw_df['score'].round()\n",
        "encoded_labels = labels.values\n",
        "\n",
        "# tokenize\n",
        "from collections import Counter\n",
        "\n",
        "print(len(review_list))\n",
        "all_words = ' '.join(review_list)\n",
        "words = all_words.split()\n",
        "\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)\n",
        "print(sorted_words[:10])\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "\n",
        "# create encoding of reviews\n",
        "reviews_int = []\n",
        "for review in review_list:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_int.append(r)\n",
        "print(reviews_int[0:3])\n",
        "print(review_list[0])\n",
        "\n",
        "\n",
        "def pad_features(reviews_int, seq_length):\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
        "\n",
        "    for i, review in enumerate(reviews_int):\n",
        "        review_len = len(review)\n",
        "\n",
        "        if review_len <= seq_length:\n",
        "            zeroes = list(np.zeros(seq_length - review_len))\n",
        "            new = zeroes + review\n",
        "        elif review_len > seq_length:\n",
        "            new = review[0:seq_length]\n",
        "\n",
        "        features[i, :] = np.array(new)\n",
        "\n",
        "    return features\n",
        "\n",
        "features = pad_features(reviews_int, 200)\n",
        "\n",
        "split_frac = 0.8\n",
        "len_feat = len(features)\n",
        "train_x = features[0:int(split_frac*len_feat)]\n",
        "train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
        "remaining_x = features[int(split_frac*len_feat):]\n",
        "remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "x = torch.from_numpy(test_x)\n",
        "y = torch.from_numpy(test_y)\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden\n",
        "\n",
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)\n",
        "\n",
        "\n",
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 1\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        print(counter)\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                inputs = inputs.type(torch.LongTensor)\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}